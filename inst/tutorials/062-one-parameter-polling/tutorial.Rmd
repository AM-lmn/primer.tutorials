---
title: 'One Parameter: Polling'
author: Ryan Southward
tutorial:
  id: one-parameter-polling
output:
  learnr::tutorial:
    progressive: yes
    allow_skip: yes
runtime: shiny_prerendered
description: Use polling data from the 2013 election to answer questions about support
  for President Obama.
---

```{r setup, include = FALSE}
library(learnr)
library(primer.tutorials)
library(tidyverse)
library(primer.data)
library(rstanarm)
library(skimr)
knitr::opts_chunk$set(echo = FALSE)
options(tutorial.exercise.timelimit = 60, 
        tutorial.storage = "local") 

# Create a fake data set with polling data.

# Harvard IOP poll w/ 2,089 respondents: 
set.seed(17)
iop_data <- tibble(response = c(rep("Approve", 855),
                                 rep("Disapprove", 1124),
                                 rep(NA, 110))) %>% 
  sample_frac()

#Class poll w/ 200 respondents
class_data <- tibble(response = c(rep("Approve", 64),
                                 rep("Disapprove", 104),
                                 rep(NA, 32))) %>% 
  sample_frac()

#Clean data to create fit. Have students use originial data set and drop NA. 

iop_clean <- iop_data %>%
  drop_na() %>%
  mutate(approve = if_else(response == "Approve", 1, 0)) %>%
  select(approve)

class_clean <- class_data %>%
  drop_na() %>%
  mutate(approve = if_else(response == "Approve", 1, 0)) %>%
  select(approve)

# Posterior creation

# iop_fit <- stan_glm(data = iop_clean,
#                     refresh = 0,
#                     seed = 2013,
#                     family = binomial,
#                     formula = approve ~ 1)
# 
# write_rds(iop_fit, "data/iop_fit.rds")

# Create model: 
iop_fit <- read_rds("data/iop_fit.rds")

# Create graph of posterior: 
ob_iop_ppd <- posterior_epred(iop_fit, 
                          newdata = tibble(constant = 1)) %>%
  as_tibble() %>%
  rename(approval = `1`) %>%
  mutate(poll = "IOP poll")

# class_fit <- stan_glm(data = class_clean,
#                     refresh = 0,
#                     seed = 2013, 
#                     family = binomial,
#                     formula = approve ~ 1) 

# write_rds(class_fit, "data/class_fit.rds")

# Create model: 
class_fit <- read_rds("data/class_fit.rds")

# Create graph of posterior: 
class_ppd <- posterior_epred(class_fit, 
                          newdata = tibble(constant = 1)) %>%
  as_tibble() %>%
  rename(approval = `1`) %>%
  mutate(poll = "Class poll")

# Predicting outcomes: 

#Posterior predictive distributions for both IOP and class polls:

iop_predictions <- posterior_predict(iop_fit, 
                                     newdata = tibble(constant = rep(1, 20))) %>%
  as_tibble() %>%
  mutate(total = rowSums(across(`1`:`20`))) %>%
  select(total) %>%
  mutate(poll = "IOP poll")

class_predictions <- posterior_predict(class_fit, 
                                     newdata = tibble(constant = rep(1, 20))) %>%
 as_tibble() %>%
  mutate(total = rowSums(across(`1`:`20`))) %>%
  select(total) %>%
  mutate(poll = "Class poll")
  
```

```{r copy-code-chunk, child = "../../child_documents/copy_button.Rmd"}

```

```{r info-section, child = "../../child_documents/info_section.Rmd"}

```

<!-- DK: Only make a plot once. Save the object and then use it twice. -->

<!-- 3) When comparing two surveys of different sizes, inferences about future samples differ much less than you might expect. Main difference is that smaller survey predicts that outlier events are (very slightly!) more like likely.  -->

<!-- RS: Because the second survey we use in this tutorial has a different value of p, it would be difficult to prove this point. -->

## Introduction
### 

In this section we will examine polling using presidential approval polls. First, you will examine data from a 2013 approval poll for President Obama conducted by the Harvard Kennedy Institute of Politics, using *Wisdom* and *Justice* to determine whether or not it can answer our questions. Next, you will calculate feasible values for the percentage of young Americans who approved of President Obama by creating a posterior distribution using the professional **rstanarm** statistical library. You will then compare the posterior of the Harvard IOP poll with another poll conducted by a government class using their centers and spreads. Finally, you will use the posteriors you create to find the expected predictions of a future situation. 

## Wisdom
### 

Before creating models, use *Wisdom* to determine whether or not the data you have is close enough to the data you need to answer your question.

### 

Our question: 

> Imagine that it is November 15, 2013. Our question: What percentage of young people (aged 18 to 29) approve of President Obama's performance?  We will use the 2013 Harvard Institute of Politics (IOP) polling data collected between October 29 and November 13 to answer the question.

### Exercise 1

In two sentences, describe the Preceptor Table which could be used to answer our question. Specify what types of people would be included in the rows, and the necessary columns of data to answer the question. As a hint, there should be two columns.

```{r wisdom-1}
question_text(NULL,
	message = "The Preceptor Table includes a row for every person in the US who is 18 to 29. It has two columns: an 'ID' column and an 'approval' column which indicates whether or not the person approved, on November 15, of President Obama's performance.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Because our question asks us for President Obama's approval rating on *November 15, 2013*, the data to fill in our Preceptor Table should come from that exact day. 

### Exercise 2

In one sentence, explain whether this model is causal or predictive, and why.

```{r wisdom-2}
question_text(NULL,
	message = "The model is predictive because there is only one potential outcome for each person: their approval (or not), November 15, 2015, of President Obama's performance.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Because our model is predictive, we only have one outcome column. 

### Exercise 3

Between October 29 and November 13 of 2013, a team from Harvard's Institute of Politics [conducted their Fall 2013 Poll](https://iop.harvard.edu/sites/default/files_new/Harvard_PressReleaseFall2013%20.pdf) **exclusively** polling 2,089 young Americans aged 18 to 29. Type `iop_data` to view the results of the poll.   


```{r wisdom-3, exercise = TRUE}

```

### 

The data we use comes from the yearly Harvard Public Opinion Project conducted by the Harvard Kennedy School Institute of Politics: 

> The Harvard Public Opinion Project conducts a biannual poll examining the political opinions and civic engagement of young Americans ages 18 to 29. Since its conception by two Harvard undergraduate students in 2000, the Harvard Public Opinion Project has provided the most comprehensive look at the political opinions, voting trends, and views on public service held by young Americans (iop.harvard.edu).

### Exercise 4

In the results we appear to see three possible responses: approve, disapprove, or NA. Let's use `skim()` from the **skimr** package on `iop_data` to confirm this. 


```{r wisdom-4, exercise = TRUE}

```

```{r wisdom-4-hint-1, eval = FALSE}
skim(...)
```

### 

Looking at n_unique for our single character column, we see that there are only 2 unique values. We also see that there are 110 missing values. This corresponds with our initial belief that there are only 3 possible values: "Approve", "Disapprove", or `NA`. Always take a thorough look at your data to make sure there are not hidden levels that are not visible at first glance. 

### Exercise 5

Although `skim()` identifies how many levels each column in our data has, it does not display the frequencies of each level. To do this, start a pipe with `iop_data`, and pipe on the function `count()`. Within `count()`, use the sole argument `response`. 


```{r wisdom-5, exercise = TRUE}

```

```{r wisdom-5-hint-1, eval = FALSE}
... %>%
  count(...)
```

### 

In effect we have created a summary of the Harvard IOP poll: Out of the 2,089 poll respondents, 855  approved of President Obama, 1124 disapproved, and answers are missing from 110. 

### Exercise 6


The final question of Wisdom is always can we can consider the Preceptor Table and the data to come from the same population? If we cannot assume *validity* across columns, meaning that that the "approval" column of the Preceptor Table does not match the meaning of the approval column of the data, then we conclude that they do not come from the same Population. 

In a paragraph, challenge the *validity* of the combining the Preceptor Table and the data together. 


```{r wisdom-6}
question_text(NULL,
	message = "The column labelled 'response' in the data is not exactly the same thing as the column labeled 'approval' in the Preceptor Table. 'response' in the data refers to what people said on the phone to strangers. This is not really what we care about! People sometimes (often?) lie to strangers! The 'approval' column in the Preceptor Table refers to people's honest internal feelings. If those are not the same thing, then the assumption of 'validity' does not hold and we can not consider the Preceptor Table and the data to have come from the same population.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

In order to assume validity and proceed with our question, we must assume that the question asked in the survey accurately gauges people's attitudes towards President Obama. We must also assume that other aspects of experimental design, such as whether or not people answered the questions truthfully, are not an issue.

## Justice
### 

The next virtue after Wisdom is Justice. Given a Population Table, we need to examine the assumptions of stability and representativeness. We also need to specify the functional form of the Data Generating Mechanism.

### Exercise 1

In one sentence, state a wider Population which encompasses both the Data and the Preceptor Table. Make sure to include demographic characteristics, as well as a specific time period. 

```{r justice-1}
question_text(NULL,
	message = "A wider population may be Americans aged 18-29 on day to day basis from October 27 to November 15, 2013.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The demographic of *young Americans aged 18-29* is important, as the poll only surveyed people aged 18-29. 

The time period for which we are examining our population is also relevant, as people's opinions change. Someone may not have had the same attitude when they were surveyed as they did a few days prior. 

### Exercise 2

<!-- DK: Make this the first question and then show the top (header) of the Population Table in the knowledge drop. -->

In one sentence, state the columns that the Population Table should have. Hint: there should be 4. 

```{r justice-2}
question_text(NULL,
	message = "ID, Source, Time, Approval",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Every Population Table has a source column that identifies if the subject is from the Data, Preceptor Table, or Population, a time column, as well as at least one outcome column. 

### Exercise 3

Describe in one sentence the rows from the Preceptor Table which are in the Population Table. Make sure to reference *time* in some capacity in your response. 

```{r justice-3}
question_text(NULL,
	message = "The Preceptor Table would have as many rows as there are 18-29 year old Americans on Nov 15, 2013, in order to make the simple algebraic calculation of President Obama's approval among all young Americans.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

It is important to state that we care about the attitudes of young Americans specifically on November 15, 2013, as that is the date the question asks us, and we acknowledge that the number of Americans aged 18-29 will change from day to day. 

### Exercise 4

Describe in one sentence the rows from the data which are in the Population Table. 

```{r justice-4}
question_text(NULL,
	message = "The Data source would have 1,979 rows, one for each poll respondent who had an opinion on wether or not they approved of President Obama.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

You may have answered that there should be 2,089 rows, which is a perfectly reasonable response. The answer is in fact 1,979 rows because the people who declined to answer (represented by NAs in our data), do not give us data which can be used to answer our question.

### Exercise 5

Describe in two sentences the rows from the wider population which are in the Population Table. Make sure to reference *time* in some capacity in your response. 

<!-- DK: Clarify the below. Maybe move the subtlties of the person turning 18 or 30 into a knowledge drop afterwards. What you write is true, but probably too much to expect a student to come up with. -->

```{r justice-5}
question_text(NULL,
	message = "The Population would be larger than the Preceptor Table and have rows for all young Americans aged 18-29, including multiple rows for a single individual as their opinion may have changed. There should also be rows for people who may turn 18 a few days after the poll, and also people who turned 30 after the poll was conducted.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

We must acknowledge that the data we have doesn't perfectly answer our question; instead, we found the best data that suits our needs and made some assumptions that it holds true for a later time period. Because we did this, we use the population to show that the data we choose could be used to answer questions over a larger time period than the day we specifically want answers for. 

### Exercise 6

In 2 sentences, what components of the Population Table would have unknown values?  

```{r justice-6}
question_text(NULL,
	message = "The approval outcome of Preceptor Table subjects would be unknown because we have no data on the attitudes of young Americans on November 15, 2013. We would know the approval outcome of 2,089 members of the Population whose data we collected in the poll, but the vast majority of young Americans approval of Obama would be unknown, including the attitudes of poll respondents both before and after the poll was conducted as their opinions could have changed.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### Exercise 7

The Harvard IOP poll was conducted between October 29 and November 13 of 2013. In 2 sentences, state a reason why the assumption of *stability* might not be met in this exercise. 

```{r justice-7}
question_text(NULL,
	message = "A major news event that happened after the poll was conducted could cause the attitudes of young adults to shift dramatically. If this were to be the case, the poll would not be representitive of the respondents more current beliefs, and instead only reflect their past attitudes before the news event.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

For presidential approval polls, time essentially creates a new population each day. For example, from October 1st to October 17th, 2013, the U.S government shut down under President Obama causing his approval rating to dip. Political events influence people's day to day beliefs, and it is fairly certain that some people would have differing attitudes about President Obama during and after the shutdown, showing that individuals from these two time periods could be considered entirely different populations. 

### Exercise 8

The [press release revealing the poll results](https://iop.harvard.edu/sites/default/files_new/Harvard_PressReleaseFall2013%20.pdf) included a brief explanation of KnowledgePanel®, which was the platform used to conduct the poll:

> The web-enabled KnowledgePanel® is a probability-based panel designed to be representative of the
U.S. population. Initially, participants are chosen scientifically by a random selection of 
telephone numbers and residential addresses. Persons in selected households are then invited by
telephone or by mail to participate in the web-enabled KnowledgePanel®. For those who agree
to participate, but do not already have Internet access, GfK provides a laptop and ISP connection
at no cost. 

With knowledge of the sampling methodology, determine one group the poll may exclude (and explain why it excludes them), which could cause the poll to not be *representative* of all 18-29 year old Americans. Respond in 1 sentence. 

```{r justice-8}
question_text(NULL,
	message = "The poll would not represent 18-29 year old prisoners, as they could not be selected via random selection of telephone numbers and addresses.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Almost no sample in the real world is ever perfectly representative. In fact, *representativeness is always a lie* to a certain extent, and for almost all surveys *some* group may be excluded. However, just because a survey is not 100% representative doesn't mean that its results can't be used to draw conclusions. 

### Exercise 9

Good. Now let's turn our attention to the Data Generating Mechanism (DGM). Should our polling scenario be modeled by a normal or binomial distribution? Explain your reasoning in 2 sentences.  

```{r justice-9}
question_text(NULL,
	message = "Our polling scenario should be modeled by a binomial distribution, because we model the number of people who approve of President Obama (successes) over people who disapprove of him (failures). Because there are only 2 possibilities to model, the poll can be modeled by a binomial distribution",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

To create our models we will use the following binomial model: 

$$ T_a \sim B(\rho , n = 2089)$$

The total number of young Americans aged 18-29 who approve of President Obama, $T_a$, is modeled by a binomial distribution with the probability of one young American approving of President Obama, $\rho$, and 2,089 observations. 

### 

Use Justice to further increase your understanding of the situation at hand. Justice always consists of a Population Table, stability, representativeness, and the Data Generating Mechanism.


## IOP Study
### 

<!-- DK: Don't express this in terms of the value of the parameter. Who cares? We don't, in general. We need to use fitted models to answer questions. Only rarely will we get a question to which the answer is the value of a parameter. -->

Whenever you want to find the value of a parameter, create a posterior distribution. 

### 

Recall our question: 

> Imagine that it is November 15, 2013. What percentage of young people (aged 18 to 29) approve of President Obama's performance?

### 

In order to answer the question, use the Harvard IOP poll to create the posterior distribution for the percentage of U.S 18-29 year olds who approved of President Obama in the fall of 2013. 

```{r}
ob_p <- ob_iop_ppd %>%
  ggplot(aes(x = approval)) +
  geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_light() +
  labs(title = "Posterior Probability Distribution",
       x = "Percentage of 18-29 year olds who approved of President Obama on Nov 15, 2013",
       y = "Probability")

ob_p 
```

### Exercise 1

In order to create a binomial model, you must structure your data so that 1's represent successes and 0's represent failures. For structuring the responses for the Obama poll, it makes sense to represent those who approved of Obama's performance with 1's, and those who disapproved with 0's. 

Run `iop_data` to view the poll results once more. 

```{r iop-study-1, exercise = TRUE}

```

### Exercise 2

Why are there some NA values? Some respondents of the Harvard IOP poll declined to answer whether or not they approved of President Obama. Assuming the absence of non-response bias, NA values should be dropped to focus our attention on the ratio between approvals and disapprovals. Start a pipe with `iop_data` and use `drop_na()`. 

```{r iop-study-2, exercise = TRUE}

```

```{r iop-study-2-hint-1, eval = FALSE}
... %>%
  drop_na()
```

### 

Your tibble should now have 1,979 rows, representing the 2,089 total poll respondents without the 110 respondents who declined to answer the question. 

### Exercise 3

Our data entries are either "Approve" or "Disapprove", instead of 1's and 0's. To change our data to a binomial format, create a new column called `approve` and set it equal to the `if_else()` function. For the first argument, pass in a logical comparison statement that checks if the `response` is equal to "Approve". For the second argument use 1 and for the third argument use 0.

```{r iop-study-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-3-hint-1, eval = FALSE}
... %>%
  mutate(... = if_else(... == ..., ..., ...))
```

```{r iop-study-3-hint-2, eval = FALSE}
... %>%
  mutate(... = if_else(... == "Approve", 1, 0))
```

### 

Rows with 1's represent respondents who approved of President Obama's performance, and 0's represent those who disapproved. 

### Exercise 4

Finally, select the `approve` column, and assign your code from above to an object named `iop_clean`.

```{r iop-study-4, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-4-hint-1, eval = FALSE}
Use the assignment operator <-  to do so.
```

### 

Excellent work. Now it's time to use the **rstanarm** package to fit a model for the data. 

### Exercise 5

Next we'll use `stan_glm()` to create a simple Bayesian model. When using `stan_glm()`, there are always four arguments that should be specified:

* The `data` argument tells `stan_glm()` what data frame to work with. Set `data` to `iop_clean`. 
* The `family` argument tells `stan_glm()` what mathematical formula should be used to create our model. Set `family` to `binomial`.
* The `formula` argument tells `stan_glm()` what model is to be fitted. Because we have no predictors, we use the argument `formula = approve ~ 1`, which means that we only model the outcome based on the `approve` column of our data. 
* Set `refresh` to 0 in order to suppress the behavior of printing to the console, and `seed` to 2013 in order to get the same output every time we run the code.

```{r iop-study-5, exercise = TRUE}

```

```{r iop-study-5-hint-1, eval = FALSE}
stan_glm(data = ...,
         family = ...,
         formula = ..., 
         refresh = ...,
         seed = ...)
```

### 

You do not need to understand what the printed model means. 

### Exercise 6

Next, assign your model to an object named `iop_fit` to save it for future use.  

```{r iop-study-6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-6-hint-1, eval = FALSE}
Use the assignment operator <-  to do so.
```

### 

It's always a good idea to save your posteriors, as they will always be utilized in predictive functions like `posterior_predict()` needed to answer our questions. 

### Exercise 7

Let's recreate our posterior distribution using `posterior_epred()`, assigning our code to a variable called `ob_iop_ppd`. Pass in `iop_fit` as the first argument, and then set the argument `newdata` equal to a `tibble()`. Within the `tibble()`, include `constant = 1`. 

```{r iop-study-7, exercise = TRUE}

```

```{r iop-study-7-hint-1, eval = FALSE}
ob_iop_ppd <- posterior_epred(..., 
                 newdata = ...)
```

### 

We use `constant = 1` as junk data to make sure our tibble has just 1 row. 

### Exercise 8

Copy your code from above. First pipe `as_tibble()` to your code. Then continue your pipe by renaming the column of the resulting tibble from `1` to `approval` by using `approval = 1` within `rename()`. **Remember to include backticks around 1**. 

```{r iop-study-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-8-hint-1, eval = FALSE}
... %>%
  ... %>%
  rename(...)
```

```{r iop-study-8-hint-2, eval = FALSE}
Within rename, set approval equal to `1`. Make sure to include backticks around 1!
```

### 

An annoying problem with `posterior_epred()` is that names its columns weirdly. We change the name of the tibble from "1" to make it less confusing. 

### Exercise 9

Finally, continue your pipe with `mutate()`. Within `mutate()`, set `poll` to "IOP poll". We uniformly assign "IOP poll" to each row in our tibble in order to compare posteriors later on. 

```{r iop-study-9, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-9-hint-1, eval = FALSE}
... %>%
  mutate(... = "IOP poll")
```

### Exercise 10

Type `ob_iop_ppd` below to view 4,000 draws from your posterior. 

```{r iop-study-10, exercise = TRUE}

```

### Exercise 11

Awesome! Now let's graph the draws from the posterior. First, start a pipe with `ob_iop_ppd`, and using `ggplot()` map `approval` to the x axis. Also add the layer `geom_histogram()`. 

```{r iop-study-11, exercise = TRUE}

```

```{r iop-study-11-hint-1, eval = FALSE}
ob_iop_ppd %>%
  ggplot(aes(... = ...)) + 
  geom_histogram()
```

### 

Because we want to find the probability of each possible Obama approval rating, we should convert our y-axis to percentage values. We should also specify the number of bins to remove the warning. 

### Exercise 12

Within `geom_histogram()`, use `aes()` to set `y` equal to `after_stat(count/sum(count))` in order to convert the y-axis to percent values. Also set `bins` to 100. 

```{r iop-study-12, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-12-hint-1, eval = FALSE}
... +
  geom_histogram(aes(... = ...),
                 bins = ...)
```

### 

What is this plot missing? Shouldn't the the number on the x and y axes be percentages instead of simple numbers?

### Exercise 13

To correct the axises,  add the layer `scale_x_continuous()` with the argument `labels` set to `scales::percent_format()`. Also add the layer `scale_y_continuous()`, with the argument `labels` also set to `scales::percent_format()`. 

```{r iop-study-13, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-13-hint-1, eval = FALSE}
... +
  scale_x_continuous(... = ...) +
  scale_y_continuous(... = ...)
```

### 

To make our graph the most presentable possible, we should remove the extra unneeded decimal places for our percentages. 

### Exercise 14

Within *both* occurrences of `scales::percent_format()`, set `accuracy` to 1 in order to limit percentages to round the first decimal place. 

```{r iop-study-14, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r iop-study-14-hint-1, eval = FALSE}
... +
  scale_x_continuous(labels = scales::percent_format(accuracy = ...)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = ...))
```

### Exercise 15

To finish, use `labs()` to give your graph the appropriate title, subtitle, and axis labels. Also add the layer `theme_light()`. 

```{r iop-study-15, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

Reminder: Your plot should look something like this:

```{r}
ob_p 
```

### Exercise 16

Using the posterior, answer the initial question in 2 sentences: 

> Imagine that it is November 15, 2013. What percentage of young people (aged 18 to 29) approve of President Obama's performance?

Because the exact value of the parameter (the percentage of young Americans who approved of Obama's performance) is unknown, your answer should not be a single number, but instead a range of values that the true value of the parameter is likely to fall between.  

```{r iop-study-16}
question_text(NULL,
	message = "The bulk of the area under the distribution occurs between 42% and 44%, so it's likely that the percentage of all young American adults who approved of President Obama is between this range. However, there is also some chance that the percentage is as low as 40%, and as high as 46.5%.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Whenever you want to find the value of a parameter, create a posterior distribution. 

<!-- ## Predicting outcomes -->

<!-- Lesson: Posterior predictions are never the truth.  -->

<!-- Problem with teaching lesson: Because our posterior is very refined, the predictions are pretty accurate. If you uncomment the code below, you will see 5% is the largest difference between actual and predicted.  -->

<!-- ```{r} -->
<!-- min(ob_iop_ppd$approval) -->
<!-- max(ob_iop_ppd$approval) -->

<!-- iop_pred <- posterior_predict(iop_fit, newdata = tibble(constant = rep(1, 10))) %>% -->
<!--   as_tibble() %>%  -->
<!--   mutate(total = rowSums(across(`1`:`10`))) %>%  -->
<!--   select(total) -->

<!-- tibble(pop_pct = seq(from = .4, to = .47, by = .01)) %>% -->
<!--   mutate(n_ppl = map(pop_pct, ~ 0:10)) %>% -->
<!--   unnest(n_ppl) %>% -->
<!--   mutate(real_prob = map_depth(.x = n_ppl, .depth = 0, ~ dbinom(x = .x, size = 10, prob = pop_pct))) %>% -->
<!--   mutate(est_prob = map_dbl(n_ppl, ~ sum(iop_pred$total == .)/4000)) %>% -->
<!--   mutate(diff = real_prob - est_prob) %>% -->
<!--   mutate(pos = ifelse(diff > 0, TRUE, FALSE)) %>% -->
<!--   ggplot(aes(x = n_ppl, y = diff, fill = pos)) +  -->
<!--   geom_col() +  -->
<!--   facet_wrap(~pop_pct) +  -->
<!--   scale_fill_manual(values = c("red", "green")) -->

<!-- ``` -->



## Smaller sample study
### 

The center of a distribution approximates a parameter for an entire population, while the spread of a distribution represents the uncertainty surrounding the estimated parameter. The sample mean or proportion determines the center, while the number of observations in the sample determines the spread.

We will use this methodology of comparing distributions to eventually compare the posterior probability distributions of the Harvard IOP poll and a new Gov't class poll:

```{r}
bind_rows(ob_iop_ppd, class_ppd) %>%
  ggplot(aes(x = approval, fill = poll)) +
  geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100, alpha = .5, position = "identity") + 
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_light() +
  labs(title = "Comparing Posterior Distributions of 2 Studies", 
       subtitle = "The differences between the studies can be expressed by
       the posterior means and spreads.",
       x = "Approval Percentage", 
       y = "Probability")
```

### 

Also between October 29 and November 13, a high school government class conducted their own Obama approval poll using the exact same methodology as the Harvard Institute of Politics poll, the only difference being that they polled 200 respondents instead of the 2,089 polled by the Harvard IOP. 

### Exercise 1

Type `class_data` to review the results of the class poll. 

```{r smaller-sample-study-1, exercise = TRUE}

```

### 

There appear to be 3 possible answers: approve, disapprove, or NA. 

### Exercise 2

Let's use the **skimr** package to see how many levels there are in the `response` column. Use `skim()`, passing in `class_data`. 

```{r smaller-sample-study-2, exercise = TRUE}

```

```{r smaller-sample-study-2-hint-1, eval = FALSE}
skim(...)
```

### 

Looking at n_unique for our single character column, we see that there are only 2 unique values. We also see that there are 32 missing values. This corresponds with our initial belief that there are only 3 possible values: approve, disapprove, or NA. Always take a thorough look at your data to make sure there are not hidden levels that are not visible at first glance. 

### Exercise 3

Although `skim()` identifies how many levels each column in our data has, it does not display the frequencies of each level. To do this, start a pipe with `class_data`, and pipe on the function `count()`. Within `count()`, use the sole argument `response`. 


```{r smaller-sample-study-3, exercise = TRUE}

```

```{r smaller-sample-study-3-hint-1, eval = FALSE}
... %>%
  count(...)
```

### 

We can now easily view the results of the class poll: Out of the 200 poll respondents, 64  approved of President Obama, 104 disapproved, and answers are missing from 32.

### Exercise 4

Because people who failed to answer the question do not provide data on their attitude towards President Obama, they should be dropped. Start a pipe with `class_data` and pipe on `drop_na()`

```{r smaller-sample-study-4, exercise = TRUE}

```

```{r smaller-sample-study-4-hint-1, eval = FALSE}
... %>%
  drop_na()
```

### 

Your tibble should now have 168 rows, representing the 200 total poll respondents without the 110 respondents who declined to answer the question. 

### Exercise 5

In order to create a binomial model, the next step is to transform the poll summary into a binomial format. Those who approved of Obama's performance should be represented with 1's, and those who disapproved with 0's. Create a new column called `approve` and set it equal to the `if_else()` function. For the first argument, pass in a logical comparison statement that checks if the `response` is equal to "Approve". For the second argument use 1 and for the third argument use 0.

```{r smaller-sample-study-5, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-5-hint-1, eval = FALSE}
... %>%
  mutate(... = if_else(... == ..., ..., ...))
```

```{r smaller-sample-study-5-hint-2, eval = FALSE}
... %>%
  mutate(... = if_else(... == "Approve", 1, 0))
```

### 

Now functions like `posterior_predict()` can understand our model. Rows with 1's represent respondents who approved of President Obama's performance, and 0's represent those who disproved. 

### Exercise 6

Assign your code from above to an object named `class_clean`.

```{r smaller-sample-study-6, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-6-hint-1, eval = FALSE}
Use the assignment operator <-  to do so.
```

### 

Excellent work. Now it's time to use the **rstanarm** package to fit a model for the data. 

### Exercise 7

Next we'll use `stan_glm()` to create a simple Bayesian model. When using `stan_glm()`, there are always four arguments that should be specified:

* The `data` argument tells `stan_glm()` what data frame to work with. Set `data` to `class_clean`. 
* The `family` argument tells `stan_glm()` what mathematical formula should be used to create our model. Set `family` to `binomial`.
* The `formula` argument tells `stan_glm()` what model is to be fitted. Because we have no predictors, we use the argument `formula = approve ~ 1`, which means that we only model the outcome based on the `approve` column of our data. 
* Set `refresh` to 0 in order to suppress the behavior of printing to the console, and `seed` to 2013 in order to get the same output every time we run the code.

```{r smaller-sample-study-7, exercise = TRUE}

```

```{r smaller-sample-study-7-hint-1, eval = FALSE}
stan_glm(data = ...,
         family = ...,
         formula = ..., 
         refresh = ...,
         seed = ...)
```

### 

You do not need to understand what the printed model means. 

### Exercise 8

Next, assign your model to an object named `class_fit` to save it for future use.  

```{r smaller-sample-study-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-8-hint-1, eval = FALSE}
Use the assignment operator <-  to do so.
```

### Exercise 9

Let's recreate our posterior distribution using `posterior_epred()`, assigning our code to a variable called `class_ppd`. Pass in `class_fit` as the first argument, and then set the argument `newdata` equal to a `tibble()`. Within the `tibble()`, include `constant = 1`. 

```{r smaller-sample-study-9, exercise = TRUE}

```

```{r smaller-sample-study-9-hint-1, eval = FALSE}
class_ppd <- posterior_epred(..., 
                 newdata = ...)
```

### 

We use `constant = 1` as junk data to make sure our tibble has just 1 row. 

### Exercise 10

Copy your code from above. First pipe `as_tibble()` to your code. Then continue your pipe by renaming the column of the resulting tibble from `1` to `approval` by using `approval = 1` within `rename()`. **Remember to include backticks around 1**. 

```{r smaller-sample-study-10, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-10-hint-1, eval = FALSE}
... %>%
  ... %>%
  rename(...)
```

```{r smaller-sample-study-10-hint-2, eval = FALSE}
Within rename, set approval equal to `1`. Make sure to include backticks around 1!
```

### Exercise 11

Finally, continue your pipe with `mutate()`. Within `mutate()`, set `poll` to "IOP poll". 

```{r smaller-sample-study-11, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-11-hint-1, eval = FALSE}
... %>%
  mutate(... = ...)
```

### 

We uniformly assign "IOP poll" to each row in our tibble in order to compare posteriors later on.

### Exercise 12

Type `class_ppd` below to view 4,000 draws from the posterior. 

```{r smaller-sample-study-12, exercise = TRUE}

```

### 

Now that we have the posterior draws for the class poll, we can work to compare it to the posterior of the IOP poll. 

### Exercise 13

Combine the Harvard IOP posterior with the class study posterior using `bind_rows()`. Include `ob_iop_ppd` as the first argument, and `class_ppd` as the second argument. 

```{r smaller-sample-study-13, exercise = TRUE}

```

```{r smaller-sample-study-13-hint-1, eval = FALSE}
bind_rows(..., ...)
```

### 

Our tibble now has 8,000 rows, half from the IOP poll posterior and half from the class poll posterior. Each row is one draw from the posterior distribution, equivalent to randomly sampling from the distribution of feasible Obama approval percentages 8,000 times. 

### Exercise 14

Copy your code from above, and continue your pipe using `ggplot()`. Map `approval` to the x-axis, and `poll` to the fill. Also add the layer `geom_histogram()`. 

```{r smaller-sample-study-14, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-14-hint-1, eval = FALSE}
... %>%
  ggplot(aes(x = ..., fill = ...)) + 
  geom_histogram()
```

### 

Because we want to find the probability of each possible Obama approval rating, we should convert our y-axis to percentage values. We should also pick a bin size to remove the warning. 

### Exercise 15

Within `geom_histogram()`, use `aes()` as the first argument to set `y` equal to `after_stat(count/sum(count))` in order to convert the y-axis to percent values. Also set `bins` to 100, `alpha` to .5, and `position` to "identity".  

```{r smaller-sample-study-15, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-15-hint-1, eval = FALSE}
... +
  geom_histogram(aes(... = ...),
                 bins = ..., 
                 alpha = ...,
                 position = ...)
```

### 

We set `position` to "identity" and `alpha` to .5 to have the bars overlap in order to visualize the blended color, and `bins` to a large number like 100 in order to add more detail to the distribution. 

### Exercise 16

Great. Now add the layer `scale_x_continuous()` with the argument `labels` set to `scales::percent_format()`. Also add the layer `scale_y_continuous()`, with the argument `labels` also set to `scales::percent_format()`.

```{r smaller-sample-study-16, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-16-hint-1, eval = FALSE}
... +
  scale_x_continuous(... = ...) +
  scale_y_continuous(... = ...)
```

### 

To make our graph the most presentable possible, we should remove the extra unneeded decimal places for our percentages. 

### Exercise 17

Within *both* occurrences of `scales::percent_format()`, set `accuracy` to 1 in order to limit percentages to round the first decimal place. 

```{r smaller-sample-study-17, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r smaller-sample-study-17-hint-1, eval = FALSE}
scale_x_continuous(labels = scales::percent_format(accuracy = ...)) +
scale_y_continuous(labels = scales::percent_format(accuracy = ...))
```

### Exercise 18

To finish, use `labs()` to give your graph the appropriate title, subtitle, and axis labels. Also add the layer `theme_light()`.

```{r smaller-sample-study-18, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

Your plot should look something like this:

```{r}
bind_rows(ob_iop_ppd, class_ppd) %>%
  ggplot(aes(x = approval, fill = poll)) +
  geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100, alpha = .5, position = "identity") + 
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_light() +
  labs(title = "Posterior Distributions of 2 polls", 
       subtitle = "Approximating the approval percentage on Nov 15, 2013",
       x = "Approval Percentage Among Young Americans aged 18-29", 
       y = "Probability",
       fill = "Poll")
```

### 

You have successfully created the posteriors for two different Obama approval polls. The next step is to compare them in terms of their centers and spreads. 

### Exercise 19

In one sentence, estimate the *center* of each distribution in your graph.

```{r smaller-sample-study-19}
question_text(NULL,
	message = "The distribution of the class poll is centered at about 38%, while the Harvard IOP poll is centered at about 43%.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### Exercise 20

Quantitatively approximate the centers of by taking the `mean()` of the `approval` column for both `ob_iop_ppd` and `class_ppd`.  

```{r smaller-sample-study-20, exercise = TRUE}

```

```{r smaller-sample-study-20-hint-1, eval = FALSE}
mean(ob_iop_ppd$...)
mean(class_ppd$...)
```

### 

Although you can determine the centers of each posterior using the graph visually, it's important to understand that the mean of the posterior determines its center. 

### Exercise 21

Now, let's compare the centers of our distribution with the approval percentages approximated by the  class and IOP polls. 

### 

Recall `iop_clean`, the "clean" Harvard IOP poll data for which we dropped `NA` values and turned the raw data into a binomial format: 

```{r}
iop_clean
```

### 

Use `sum()` to find the number of times the `approve` column of `iop_clean` is equal to `1`. Divide that quantity by the number of rows of `iop_clean`, for which you can use `nrow()` to calculate. 

```{r smaller-sample-study-21, exercise = TRUE}

```

```{r smaller-sample-study-21-hint-1, eval = FALSE}
sum(... == ...)/nrow(...)
```

```{r smaller-sample-study-21-hint-2, eval = FALSE}
sum(iop_clean$... == 1)/nrow(...)
```

### 

The Harvard IOP poll approximated that `r round(100*(sum(iop_clean$approve == 1)/nrow(iop_clean)), digits = 3)`% of young Americans aged 18-29 approved of President Obama. 

### Exercise 22

Now approximate the approval percentage approximated by the class poll. Recall `class_clean`, for which we "cleaned" the smaller class poll data: 

```{r}
class_clean
```

### 

Use `sum()` to find the number of times the `approve` column of `class_clean` is equal to `1`. Divide that quantity by the number of rows of `class_clean`, for which you can use `nrow()` to calculate. 

```{r smaller-sample-study-22, exercise = TRUE}

```

```{r smaller-sample-study-22-hint-1, eval = FALSE}
sum(... == ...)/nrow(...)
```

```{r smaller-sample-study-22-hint-2, eval = FALSE}
sum(class_clean$... == 1)/nrow(...)
```

### 

The class government poll approximated that `r round(100*(sum(class_clean$approve == 1)/nrow(class_clean)), digits = 3)`% of young Americans aged 18-29 approved of President Obama. 

### Exercise 23

Using the previous few exercises, what determines the *center* of a distribution? Respond in one sentence.

```{r smaller-sample-study-23}
question_text(NULL,
	message = "The sample approximation of a population parameter determines the center of a distribution. ",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

Know that a sample mean or proportion determines the center of empirical *normal distributions* specifically. Whenever you see the graph of a distribution created from data, think about what the center says about the data used to create the graph. 



### Exercise 24

We can also compare *variation* in distributions from their *spreads*. 

```{r}
bind_rows(ob_iop_ppd, class_ppd) %>%
  ggplot(aes(x = approval, fill = poll)) +
  geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100, alpha = .5, position = "identity") + 
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_light() +
  labs(title = "Posterior Distributions of 2 polls", 
       subtitle = "Approximating the approval percentage on Nov 15, 2013",
       x = "Approval Percentage Among Young Americans aged 18-29", 
       y = "Probability",
       fill = "Poll")
```

In 2 sentences, compare the shapes of the distributions in terms of their *spreads*.

```{r smaller-sample-study-24}
question_text(NULL,
	message = "The distribution of the IOP poll is tall and narrow meaning that it has little spread.  Comparitively,  the class poll is short and wide meaning it has a large spread.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### Exercise 25

What does the difference in spreads of the Harvard IOP and class poll distributions tell you? Respond in 1 sentence, making sure to use the term *uncertainty* in your response. 

```{r smaller-sample-study-25}
question_text(NULL,
	message = "The class poll has much more uncertainty regarding the true percentage of young Americans aged 18-29 who approve of President Obama because its spread is much larger than that of the Harvard IOP poll.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### Exercise 26

Calculate the variation of each distribution by finding the standard deviation using `sd()` of the `approval` column for both `ob_iop_ppd` and `class_ppd`. 

```{r smaller-sample-study-26, exercise = TRUE}

```

```{r smaller-sample-study-26-hint-1, eval = FALSE}
sd(ob_iop_ppd$...)
sd(class_ppd$...)
```

### 

It is important to realize that the standard deviation quantifies the variation of a distribution, giving its spread. Because the standard deviation of the class poll is 3 times that of the Harvard IOP poll, it has much larger spread. 

### Exercise 27

Consider how the number of *observations* in each poll may influence the spread. Using `nrow()`, find the number of rows in both `iop_clean` and `class_clean` to determine the number of observations each poll had (without `NA` values).

```{r smaller-sample-study-27, exercise = TRUE}

```

```{r smaller-sample-study-27-hint-1, eval = FALSE}
nrow(...)
nrow(...)
```

```{r smaller-sample-study-27-hint-2, eval = FALSE}
nrow(iop_clean)
nrow(class_clean)
```

### 

The Harvard IOP poll had `r nrow(iop_clean)` observations, while the smaller class poll had more than 11 times less less observations with `r nrow(class_clean)`. 

### Exercise 28

Using the previous few exercises, what causes a distribution to have more or less *spread*? Respond in 1 sentence, making sure to use the term *observations* in your response. 

```{r smaller-sample-study-28}
question_text(NULL,
	message = "Distributions approximated with many observations have little spread, while ditributions approximated with few observations have more spread.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

There is an intuitive relationship between *spread* and the number of *observations*. With few people in a sample you are uncertain if the sample is representative of the entire population, and as such the distribution will have wider spread. As the number of people in the sample increases, then you are more certain in your estimate of the population parameter, as a larger sample is more likely to "cancel out" extreme values. 

### 

Whenever you want to describe a distribution, consider its spread and center. The center of a distribution approximates a parameter for an entire population, while the spread of a distribution represents the uncertainty surrounding the estimated parameter. Use the sample mean or proportion to find the center, and the number of observations in the sample to determine the spread. 

## Simple prediction
### 

Posterior predictive draws have certain properties: 

- With a large enough sample size, posterior predictive distributions are normally distributed.

- The sample proportion used to create the posterior has a large effect on the predictive draws, while the number of observations used to create the posterior has a small effect.

### 

To illustrate the above properties, we will answer the following question: 

> Imagine that it is November 15, 2013. For the wider population associated with the Harvard IOP and Gov't class polls, how many people would approve of President Obama in a room of 20 people? 

### 

To answer this question, we must use `posterior_predict()` for both the IOP and Gov't class polls, using 20 observations, and compare the results:

```{r}
bind_rows(iop_predictions, class_predictions) %>%
  ggplot(aes(x = total, fill = poll)) +
  geom_histogram(aes(y = after_stat(count/sum(count))), position = "dodge", alpha = .5, bins = 50) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(title = "Posterior Probability Distribution",
       subtitle = "Number of Obama approvers in a room of 20 people for 2 different polls",
       y = "Probability",
       x = "Number of Obama Approvers in Room",
       fill = "Poll")
```

### Exercise 1

First, let's create the posterior predictions for the IOP poll. 

Within the `posterior_predict()` function, pass in `iop_fit` as the first argument, and then set the argument `newdata` equal to a `tibble()`. Within the `tibble()`, set `constant` to a vector that repeats the number 1, 20 times. Hint: use the function `rep()` to repeat, passing what you want to repeat as the first argument, and the number of times you want to repeat it as the second argument. 

Assign your code to a variable named `iop_predictions`. 

```{r simple-prediction-1, exercise = TRUE}

```

```{r simple-prediction-1-hint-1, eval = FALSE}
iop_predictions <- posterior_predict(..., 
                  ... = tibble(constant = ...))
```

```{r simple-prediction-1-hint-2, eval = FALSE}
iop_predictions <- posterior_predict(..., 
                  ... = tibble(constant = rep(..., ...)))
```

### 

We use pass a tibble with 20 rows into `newdata` because we want to estimate the number of red draws with a shovel size of 20. 

### Exercise 2

Copy your code from above and pipe on `as_tibble()`. Then type `iop_predictions` to view the results. 

```{r simple-prediction-2, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-2-hint-1, eval = FALSE}
... %>%
  as_tibble()
```

### 

Each of the 4,000 rows represents one posterior prediction of the number of Obama approvers in a room of size 20. Each column represents the 20 individuals in the room, with 0's representing people who disapprove of President Obama, and 1's representing people who approve of him. 

### Exercise 3

Let's find the total number of people who approve of Obama in each prediction. Copy your code from above and create a column named `total` using `mutate()`. Set `total` to `rowSums(across(1:20))`, **making sure to surround both the 1 and 20 in back ticks!** Then continue your pipe and select the `total` column.

```{r simple-prediction-3, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-3-hint-1, eval = FALSE}
... %>%
  mutate(total = ...) %>%
  select(...)
```

### 

Instead of having 20 columns, one for the results of each individual, using `rowSums()` allows us to find the sum across our rows and only with the total number of Obama approvers. 

### Exercise 4

Finally, add another column called `poll`, and assign all rows the value "IOP poll" so we can compare poll predictions later on. 


```{r simple-prediction-4, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-4-hint-1, eval = FALSE}
mutate(poll = ...)
```

### Exercise 5

Great work! Now let's create the posterior predictions for the Gov't class poll. 

Because most of the code is the same, copy your code from above, making sure to change a few key features. First, change the variable name from `iop_predictions` to `class_predictions`. Next, replace `iop_fit` with `class_fit`. Finally, change the uniformly assigned name in the `poll` column from "IOP poll" to "Class poll". 

Type `class_predictions` to view the results. 

```{r simple-prediction-5, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

### 

Great work! You now have the posterior predictions for the IOP poll saved as `iop_predictions`, and the predictions from the Gov't class poll saved as `class_predictions`. 

### Exercise 6

Now combine the two polls into one tibble using `bind_rows()`. Pass in `iop_predictions` as the first argument, and `class_predictions` as the second. 

```{r simple-prediction-6, exercise = TRUE}

```

```{r simple-prediction-6-hint-1, eval = FALSE}
bind_rows(..., ...)
```

### 

Our tibble now has 8,000 rows, half from the IOP poll predictions and half from the class poll predictions. Again, each row can be thought of as a simulation of a room of 20 people, with the `total` column displaying the predicted number of Obama approvers in that room. 

### Exercise 7

Now let's graph our results. Use `ggplot()` on your code from above, and map `total` to the x-axis and `poll` to fill. Add the layer `geom_histogram()`

```{r simple-prediction-7, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-7-hint-1, eval = FALSE}
... %>%
  ggplot(aes(x = ..., fill = ...)) + 
  geom_histogram()
```

### 

Because we are using `geom_histogram()`, bar height will be calculated by the number of observations for each value of `total`. 

### Exercise 8

Within `geom_histogram()`, use `aes()` to set `y` equal to `after_stat(count/sum(count))` in order to convert the y-axis to percent values. Also set `bins` to 50 and `position` to "dodge". 

```{r simple-prediction-8, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-8-hint-1, eval = FALSE}
... +
  geom_histogram(aes(y = ...),
                 bins = ...,
                 alpha = ...,
                 position = ...)
```

### 

Although there are only 20 possible x-values, we set the `bins` to 50 in order to create space between the bars. 

### Exercise 9

Another issue is that while we are attempting to create a posterior probability distribution, the y-axis is formatted as decimals instead of percents.

Add the layer `scale_y_continuous()` with the argument `labels` set to `scales::percent_format()`. Within `scales::percent_format()`, set `accuracy` to 1. 

```{r simple-prediction-9, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

```{r simple-prediction-9-hint-1, eval = FALSE}
... +
   scale_y_continuous(labels = ...)
```

```{r simple-prediction-9-hint-2, eval = FALSE}
... +
   scale_y_continuous(labels = scales::percent_format(accuracy = ...))
```

### Exercise 10

To finish, use `labs()` to give your graph the appropriate title, subtitle, and axis labels. Also add the layer `theme_light()`.

```{r simple-prediction-10, exercise = TRUE}

```

<button onclick = "transfer_code(this)">Copy previous code</button>

Your plot should look something like this:

```{r}
bind_rows(iop_predictions, class_predictions) %>%
  ggplot(aes(x = total, fill = poll)) +
  geom_histogram(aes(y = after_stat(count/sum(count))), position = "dodge", alpha = .5, bins = 50) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(title = "Posterior Probability Distribution",
       subtitle = "Number of Obama approvers in a room of 20 people for 2 different polls",
       y = "Probability",
       x = "Number of Obama Approvers in Room",
       fill = "Poll")
```

### 

Great work! Now let's use our graph to answer the question we began with: 

> Imagine that it is November 15, 2013. For the wider population associated with the Harvard IOP and Gov't class polls, how many people would approve of President Obama in a room of 20 people? 

### Exercise 11

In 1 sentence, use evidence from your plot to support the following property of posterior predictive distributions: 

> With a large enough sample size, posterior predictive distributions are normally distributed.


```{r simple-prediction-11}
question_text(NULL,
	message = "Both posterior predictive distributions are bell-shaped, as they start with values with low probabilities, then the probability increases with each subsequent number of expected Obama approvers, and then decrease. ",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 

The number of trials influences how "normal" the graph appears. If we decided to increase the number of posterior predictive draws, then our distribution would appear like an even more symmetrical normal distribution

### Exercise 12

Using evidence from your graph, how do the centers and spreads of the two distributions compare? Respond in 2 sentences. 

```{r simple-prediction-12}
question_text(NULL,
	message = "The distributions are centered differently, as the class poll is centered at predicting 8 Obama approvers in the room, while the Harvard IOP poll is centered at predicting 9 Obama approvers in the room. However, the spreads of the distributions are extremely similar, as both graphs appear to have the same shape.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### Exercise 13

Use your answer from above to support the following claim in two sentences: 

> The sample proportion used to create the posterior has a large effect on the predictive draws, while the number of observations used to create the posterior has a small effect.


```{r simple-prediction-13}
question_text(NULL,
	message = "The sample proportion determines the probability of success in a binomial distribution, which as an effect deterimes where the posterior predictive distributions are centered. On the other hand, the spread of the posterior predictive distributions are similar, which demonstrates that the number of observations used to create the posterior has little effect.",
	answer(NULL,
	correct = TRUE),
	allow_retry = FALSE,
	incorrect = NULL,
	rows = 6)
```

### 


Posterior predictive draws have certain properties: 

- With a large enough sample size, posterior predictive distributions are normally distributed.

- The sample proportion used to create the posterior has a large effect on the predictive draws, while the number of observations used to create the posterior has a small effect.

<!-- ## Comparing number of observations  -->

<!-- I decided to delete the section of showing the predictive posterior distributions for different numbers of observations, but maybe the code could be of use to a future editor? -->

<!-- The following section illustrates two concepts (uncomment the plot and run it):  -->

<!-- - Only the center of each distribution, not the spread, is effected by changing the number of observations, showing that the spread has little effect on posterior predictions -->
<!-- - As you increase the # of observations, the distributions become more normal.  -->

<!-- The first bullet point can be illustrated with just one room size, as the distributions have a similar spread. I determined the second bullet point is not important enough to warrant its own section.  -->

<!-- ```{r} -->
<!-- both_polls %>% -->
<!--   pivot_longer(names_to = "group", values_to = "num_draws", cols = 3:4) %>% -->
<!--   ggplot(aes(x = total, y = num_draws, fill = group)) +  -->
<!--   facet_wrap(~num_people) + -->
<!--   geom_col(position = "dodge", alpha = .5) + -->
<!--   scale_fill_manual(values = c("#e01919", "#213fd1")) +  -->
<!--   labs(title = "Similarities and Differences for 2 Different Rooms", -->
<!--        subtitle = "Each frame represents a different room size", -->
<!--        y = "Number of Posteiror Draws", -->
<!--        x = "Number of Obama Approvers in Room") -->
<!-- ``` -->


<!-- ### Exercise 1 -->

<!-- Because the probability of the two rooms being identical is contingent on room size, we will make posterior predictions for 20 different room sizes: from 1 person, all the way to 20 people. -->

<!-- Initialize a `tibble()`, with the column `num_people` equal to a range of integers from 1 to 20 -->

<!-- ```{r probability-1, exercise = TRUE} -->


<!-- ``` -->

<!-- ```{r probability-1-hint-1, eval = FALSE} -->
<!-- tibble(... = ...:...) -->
<!-- ``` -->

<!-- ### Exercise 2 -->

<!-- Now create a column called `predictions` using `mutate()`. Set `predictions` equal to `map()`, using `num_people` for the first argument, and a ~ followed by `posterior_predict()` for the second.  -->

<!-- Within `posterior_predict()` pass in our `iop_fit` model as the first argument, and set the second argument `newdata` equal to a `tibble()`.  -->

<!-- Within the `tibble()`, set `constant` to the `rep()` function, passing in the number 1 as the first argument, and a `.` as the second.  -->

<!-- ```{r probability-2, exercise = TRUE} -->


<!-- ``` -->

<!-- <button onclick = "transfer_code(this)">Copy previous code</button> -->

<!-- ```{r probability-2-hint-1, eval = FALSE} -->
<!-- ... %>% -->
<!--   mutate(... = map(..., ~ ...)) -->
<!-- ``` -->

<!-- ```{r probability-2-hint-2, eval = FALSE} -->
<!-- Within map use posterior_predict(): -->
<!-- posterior_predict(..., newdata = tibble(... = rep(..., ...))) -->
<!-- ``` -->

<!-- ###  -->

<!-- For each room size we use the `map()` function to pass the room size into `posterior_predict()`, creating posterior predictions for each room size.  -->

<!-- ### Exercise 3 -->

<!-- Copy your code from above. Create a new column called `total` and set it equal to `map()`. Within `map()`, use `predictions` as the first argument, and a ~ followed by `rowSums()` as the second. Pass the `.` operator into `rowSums()`. -->

<!-- ```{r probability-3, exercise = TRUE} -->

<!-- ``` -->

<!-- <button onclick = "transfer_code(this)">Copy previous code</button> -->

<!-- ```{r probability-3-hint-1, eval = FALSE} -->
<!-- ... %>% -->
<!--   mutate(... = map(..., ~ ...)) -->
<!-- ``` -->

<!-- ###  -->

<!-- We use `rowSums()` in order to calculate the total number of people who approve of Obama within a room.  -->

<!-- ### Exercise 4 -->

<!-- Continue your pipe with the function `unnest()`. Pass in `total` as the sole argument.  -->

<!-- ```{r probability-4, exercise = TRUE} -->

<!-- ``` -->

<!-- <button onclick = "transfer_code(this)">Copy previous code</button> -->

<!-- ```{r probability-4-hint-1, eval = FALSE} -->
<!-- ... %>% -->
<!--   unnest(...) -->
<!-- ``` -->

<!-- ###  -->

<!-- The `unnest()` function expands `total` out of list form into individual integers.  -->

<!-- ### Exercise 5 -->

<!-- Now group your tibble by `num_people` and `total` using the `group_by()` function.  -->

<!-- ```{r probability-5, exercise = TRUE} -->

<!-- ``` -->

<!-- <button onclick = "transfer_code(this)">Copy previous code</button> -->

<!-- ```{r probability-5-hint-1, eval = FALSE} -->
<!-- ... %>% -->
<!--   group_by(..., ...) -->
<!-- ``` -->

<!-- ### Exercise 6 -->

<!-- Continue your pipe one last time using the `summarize()` function. Within `summarize()`, set `num_draws` to `n()`. Also set the `.groups` argument to "drop".  -->

<!-- ```{r probability-6, exercise = TRUE} -->

<!-- ``` -->

<!-- <button onclick = "transfer_code(this)">Copy previous code</button> -->

<!-- ```{r probability-6-hint-1, eval = FALSE} -->
<!-- ... %>% -->
<!--   summarize(... = ..., ... = ...) -->
<!-- ``` -->

<!-- ###  -->

<!-- In essence, the 4,000 draws from `posterior_predict()` represent 4,000 simulations of one room size. The `n()` function tallies the predicted number of Obama approvers across for each room size across all the simulations.   -->

<!-- ### Exercise 7 -->

<!-- Assign your code from above to an object named `iop_rooms`. -->

<!-- ```{r probability-7, exercise = TRUE} -->

<!-- ``` -->

<!-- <button onclick = "transfer_code(this)">Copy previous code</button> -->

<!-- ```{r probability-7-hint-1, eval = FALSE} -->
<!-- Use the assignment operator <-  to do so. -->
<!-- ``` -->

<!-- ###  -->

<!-- The `iop_rooms` tibble represents the predictions for the Harvard IOP poll.   -->

<!-- ### Exercise 8 -->

<!-- To simulate the predictions for the Gov't class poll we can recreate the exact same tibble as above, but using the posterior from the class survey instead of the posterior from the IOP survey.  -->

<!-- ###  -->

<!-- Copy your code from above, and replace `iop_fit` with `class_fit`. Also save the object with the name `class_rooms` instead of `iop_rooms`.  -->

<!-- ```{r probability-8, exercise = TRUE} -->

<!-- ``` -->

<!-- <button onclick = "transfer_code(this)">Copy previous code</button> -->


<!-- ###  -->

<!-- We now have a tibble named `iop_rooms` with the predictions from the IOP poll, and a tibble named `class_rooms` with the predictions for the Gov't class poll.  -->

<!-- ### Exercise 9 -->

<!-- To compare the predictions for young Americans with the predictions for all Americans, join the tibbles together using `inner_join()`. Pass in `class_rooms` as the first argument and `iop_rooms` as the second. For the third argument, set `by` equal to a vector with 2 elements, "num_people" and "total".  -->

<!-- ```{r probability-9, exercise = TRUE} -->

<!-- ``` -->

<!-- ```{r probability-9-hint-1, eval = FALSE} -->
<!-- inner_join(..., ..., ... = c(..., ...)) -->
<!-- ``` -->

<!-- ### Exercise 10 -->

<!-- Copy your code from above. Use the `rename()` function to rename the column `num_draws.x` to `class_draws`, and `num_draws.y` to `iop_draws`. Remember to use the new_name = old_name syntax.  -->

<!-- ```{r probability-10, exercise = TRUE} -->

<!-- ``` -->

<!-- <button onclick = "transfer_code(this)">Copy previous code</button> -->

<!-- ```{r probability-10-hint-1, eval = FALSE} -->
<!-- ... %>% -->
<!--   rename(... = ..., ... = ...) -->
<!-- ``` -->

<!-- ### Exercise 11 -->

<!-- Save your tibble to a variable called `both_polls`. -->

<!-- ```{r probability-11, exercise = TRUE} -->

<!-- ``` -->

<!-- <button onclick = "transfer_code(this)">Copy previous code</button> -->

<!-- ```{r probability-11-hint-1, eval = FALSE} -->
<!-- Use the assignment operator <-  to do so. -->
<!-- ``` -->

<!-- ### Exercise 12 -->

<!-- RS: Need exercises constructing the beginning plot w/ ggplot -->

## Summary
### 

By completing this tutorial, you have successfully conquered multiple real world problems like a professional. You first analyzed the Data and Preceptor Table in *Wisdom* and the Population Table in *Justice* to determine whether or not the data can answer the questions. Next, you created a posterior for the Harvard IOP survey to quantify your uncertainty about the true value of a population parameter.  You then learned to describe distributions in terms of their means and variations in order to accurately describe their differences. Finally, you used your posteriors in order to predict the results of future situations. 

### 

Great work!

```{r download-answers, child = "../../child_documents/download_answers.Rmd"}

```
